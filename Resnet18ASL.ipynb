{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "iouJ7Btcv8b_",
        "outputId": "22f195c1-f4ee-4660-84a6-0689d0b5697a"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyZO2N5Jv5fu",
        "outputId": "3455ad29-c2fd-4117-ae31-b66c9d803a49"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fw9_PLJvwkE",
        "outputId": "76ec1983-05f2-41da-df9c-237b6b7460bd"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!kaggle datasets download -d grassknoted/asl-alphabet\n",
        "\n",
        "# Extract\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "datasets = [ 'asl-alphabet.zip']\n",
        "for dataset in datasets:\n",
        "    with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
        "        zip_ref.extractall('data')\n",
        "asl_path = \"data/asl_alphabet_train/asl_alphabet_train\"\n",
        "selected_classes = ['F', 'G', 'H', 'I', 'R', 'O', 'Q', 'T', 'U', 'V']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNsH2bmssGTo",
        "outputId": "f09a1bf2-b9cd-4d01-b58c-f7cf73b896e4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "selected_classes = ['F', 'G', 'H', 'I', 'R', 'O', 'Q', 'T', 'U', 'V']\n",
        "\n",
        "# Path to the dataset\n",
        "asl_train_path = \"data/asl_alphabet_train/asl_alphabet_train\"\n",
        "\n",
        "# Create a new folder to store the filtered data\n",
        "filtered_train_path = 'data/filtered_train'\n",
        "os.makedirs(filtered_train_path, exist_ok=True)\n",
        "\n",
        "for class_name in selected_classes:\n",
        "    class_folder = os.path.join(asl_train_path, class_name)\n",
        "    if os.path.exists(class_folder):\n",
        "        target_folder = os.path.join(filtered_train_path, class_name)\n",
        "        os.makedirs(target_folder, exist_ok=True)\n",
        "\n",
        "        for image_name in os.listdir(class_folder):\n",
        "            image_path = os.path.join(class_folder, image_name)\n",
        "            shutil.copy(image_path, target_folder)\n",
        "\n",
        "\n",
        "print(f\"Filtered dataset stored at: {filtered_train_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOlJU8712JRO",
        "outputId": "bd860bba-acf0-4a3f-c2ee-2759b31f2c09"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPySNpTI7-YW"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import os\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxakOuQk8Ae4"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "full_dataset = datasets.ImageFolder(root='data/filtered_train', transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "0a_4lfLst1GT",
        "outputId": "d631acb8-4f4d-45f3-e159-be93df12fcaa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "# Load the pre-trained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "num_classes = len(selected_classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # You can adjust this based on your requirements\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move the data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    # Calculate accuracy for the epoch\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "    # Optionally: You can validate the model after each epoch\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():  # No gradients needed during validation\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        print(f\"Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf0Yy4Ui1oek"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/My Drive/resnet18_asl_model.pth'\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfo_svNyiD03",
        "outputId": "65f58dff-4b0b-4fa1-daf3-c659bef048c0"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# تحميل صورة جديدة\n",
        "image_path = '/content/data/asl_alphabet_train/asl_alphabet_train/F/F1.jpg'  # ضع هنا مسار الصورة\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# تطبيق التحويلات على الصورة\n",
        "image = transform(image).unsqueeze(0)  # إضافة بعد إضافي ليكون الشكل (1, 3, 224, 224)\n",
        "\n",
        "# التأكد من أن الصورة على نفس الجهاز مثل النموذج (GPU أو CPU)\n",
        "image = image.to(device)\n",
        "\n",
        "# ضع النموذج في وضع التقييم\n",
        "model.eval()\n",
        "\n",
        "# التنبؤ\n",
        "with torch.no_grad():  # عدم حساب التدرجات\n",
        "    outputs = model(image)  # تمرير الصورة عبر النموذج\n",
        "    _, predicted = torch.max(outputs, 1)  # الحصول على الفئة المتنبأ بها\n",
        "# معجم من الأرقام إلى الحروف (قم بتعديل هذا حسب ترتيب الفئات في مشروعك)\n",
        "class_to_label = {\n",
        "    0: 'F',\n",
        "    1: 'G',\n",
        "    2: 'H',\n",
        "    3: 'I',\n",
        "    4: 'R',\n",
        "    5: 'O',\n",
        "    6: 'Q',\n",
        "    7: 'T',\n",
        "    8: 'U',\n",
        "    9: 'V',\n",
        "}\n",
        "\n",
        "# التنبؤ\n",
        "with torch.no_grad():  # عدم حساب التدرجات\n",
        "    outputs = model(image)  # تمرير الصورة عبر النموذج\n",
        "    _, predicted = torch.max(outputs, 1)  # الحصول على الفئة المتنبأ بها\n",
        "\n",
        "# الحصول على الحرف المتنبأ به\n",
        "predicted_class = class_to_label[predicted.item()]\n",
        "\n",
        "# عرض الفئة المتنبأ بها\n",
        "print(f\"Predicted Class: {predicted_class}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpCIR1yicvZ8"
      },
      "outputs": [],
      "source": [
        "test_path=\"data/asl_alphabet_test\"\n",
        "test_dataset=datasets.ImageFolder(root=test_path, transform=transform)\n",
        "test_loader=DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yUoeVP8VdKWF",
        "outputId": "3579b8c3-b3d6-4c6a-c189-a4d37bfd5413"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# تحميل البيانات المفلترة لمجموعة الاختبار\n",
        "test_dataset = datasets.ImageFolder(root=test_path, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# الآن يمكنك اختبار النموذج على هذه البيانات المفلترة كما كنت تفعل سابقًا\n",
        "model.eval()  # ضع النموذج في وضع التقييم\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "# تخزين الصور والعلامات للتنبؤ\n",
        "images_for_display = []\n",
        "labels_for_display = []\n",
        "predictions_for_display = []\n",
        "\n",
        "with torch.no_grad():  # لا نحتاج لحساب التدرجات أثناء الاختبار\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # التنبؤات\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # التنبؤات الصحيحة\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "        # حفظ بعض الصور لعرضها مع التنبؤات\n",
        "        images_for_display.extend(inputs.cpu().numpy())\n",
        "        labels_for_display.extend(labels.cpu().numpy())\n",
        "        predictions_for_display.extend(predicted.cpu().numpy())\n",
        "\n",
        "# حساب الدقة على بيانات الاختبار\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# عرض بعض الأمثلة مع التنبؤات\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # إلغاء التحجيم\n",
        "    np_img = img.numpy()\n",
        "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# طباعة بعض الأمثلة\n",
        "for i in range(5):  # عرض 5 أمثلة\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    imshow(torchvision.utils.make_grid(torch.tensor(images_for_display[i])))\n",
        "\n",
        "    print(f\"Actual Label: {test_dataset.classes[labels_for_display[i]]}\")\n",
        "    print(f\"Predicted Label: {test_dataset.classes[predictions_for_display[i]]}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Real time _Resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=False)  \n",
        "model.fc = torch.nn.Linear(in_features=512, out_features=10)\n",
        "\n",
        "state_dict = torch.load(\"models/resnet18_asl_model.pth\",map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "model.to('cpu')\n",
        "\n",
        "classes = ['F', 'G', 'H', 'I', 'R', 'O', 'Q', 'T', 'U', 'V']\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    h, w, _ = frame.shape\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(rgb_frame)\n",
        "\n",
        "    if results.multi_hand_landmarks:\n",
        "        for hand_landmarks in results.multi_hand_landmarks:\n",
        "            x_coords = [landmark.x * w for landmark in hand_landmarks.landmark]\n",
        "            y_coords = [landmark.y * h for landmark in hand_landmarks.landmark]\n",
        "            x_min, x_max = int(min(x_coords)), int(max(x_coords))\n",
        "            y_min, y_max = int(min(y_coords)), int(max(y_coords))\n",
        "\n",
        "            margin = 20\n",
        "            x_min = max(x_min - margin, 0)\n",
        "            y_min = max(y_min - margin, 0)\n",
        "            x_max = min(x_max + margin, w)\n",
        "            y_max = min(y_max + margin, h)\n",
        "\n",
        "            hand_img = frame[y_min:y_max, x_min:x_max]\n",
        "\n",
        "            if hand_img.size != 0:\n",
        "                pil_img = Image.fromarray(cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB))\n",
        "                input_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output = model(input_tensor)\n",
        "                    pred = output.argmax(dim=1).item()\n",
        "                    label = classes[pred]\n",
        "\n",
        "                cv2.putText(frame, f\"Pred: {label}\", (x_min, y_min - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "    cv2.imshow(\"ASL Detection\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
